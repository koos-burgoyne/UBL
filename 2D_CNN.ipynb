{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdebafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy library for use in ...\n",
    "import numpy as np\n",
    "\n",
    "amino_acids = {\n",
    "    \"A\":0, \"R\":1, \"N\":2, \"D\":3, \"C\":4, \"Q\":5, \"E\":6, \"G\":7, \"H\":8, \"I\":9, \"L\":10, \"K\":11, \"M\":12, \"F\":13, \"P\":14, \"S\":15, \"T\":16, \"W\":17, \"Y\":18, \"V\":19\n",
    "}\n",
    "\n",
    "VHSE_descriptors = {\n",
    "    0:{\"A\":0.15, \"R\":-1.47, \"N\":-0.99, \"D\":-1.15, \"C\":0.18, \"Q\":-.96, \"E\":-1.18, \"G\":-0.2, \"H\":-0.43, \"I\":1.27, \"L\":11.36, \"K\":-1.17, \"M\":1.01, \"F\":1.52, \"P\":0.22, \"S\":-0.67, \"T\":-0.34, \"W\":1.5, \"Y\":0.61, \"V\":0.76},\n",
    "    1:{\"A\":-1.11, \"R\":1.45, \"N\":0,     \"D\":0.67, \"C\":-1.67, \"Q\":0.12, \"E\":0.4, \"G\":-1.53, \"H\":-0.25, \"I\":-0.14, \"L\":0.07, \"K\":0.7, \"M\":-0.53, \"F\":0.61, \"P\":-0.17, \"S\":-0.86, \"T\":-0.51, \"W\":2.06, \"Y\":1.6, \"V\":-0.92},\n",
    "    2:{\"A\":-1.35, \"R\":1.24, \"N\":-0.37, \"D\":-0.41, \"C\":-0.46, \"Q\":0.18, \"E\":0.1, \"G\":-2.63, \"H\":0.37, \"I\":0.3, \"L\":0.26, \"K\":0.7, \"M\":0.43, \"F\":0.96, \"P\":-0.5, \"S\":-1.07, \"T\":-0.55, \"W\":1.79, \"Y\":1.17, \"V\":0.17},\n",
    "    3:{\"A\":-0.92, \"R\":1.27, \"N\":0.69, \"D\":-0.01, \"C\":-.021, \"Q\":0.16, \"E\":0.36, \"G\":2.28, \"H\":0.19, \"I\":-1.8, \"L\":-0.8, \"K\":0.8, \"M\":0, \"F\":-0.16, \"P\":0.05, \"S\":-0.41, \"T\":-1.06, \"W\":0.75, \"Y\":0.73, \"V\":-1.91},\n",
    "    4:{\"A\":0.02, \"R\":1.55, \"N\":-0.55, \"D\":-2.68, \"C\":0, \"Q\":0.09, \"E\":-2.16, \"G\":-0.53, \"H\":0.51, \"I\":0.3, \"L\":0.22, \"K\":1.64, \"M\":0.23, \"F\":0.25, \"P\":-0.01, \"S\":-0.32, \"T\":0.01, \"W\":0.75, \"Y\":0.53, \"V\":0.22},\n",
    "    5:{\"A\":-0.91, \"R\":1.47, \"N\":0.85, \"D\":1.31, \"C\":1.2, \"Q\":0.42, \"E\":-0.17, \"G\":-1.18, \"H\":1.28, \"I\":-1.61, \"L\":-1.37, \"K\":0.67, \"M\":0.1, \"F\":0.28, \"P\":-1.43, \"S\":0.27, \"T\":-0.01, \"W\":-0.13, \"Y\":0.25, \"V\":-1.4},\n",
    "    6:{\"A\":0.36, \"R\":1.3, \"N\":0.73, \"D\":0.03, \"C\":-1.61, \"Q\":-0.2, \"E\":0.91, \"G\":2.01, \"H\":0.93, \"I\":-0.16, \"L\":0.08, \"K\":1.63, \"M\":-0.86, \"F\":-1.33, \"P\":-0.19, \"S\":-0.64, \"T\":-0.79, \"W\":-1.06, \"Y\":-0.96, \"V\":-0.24},\n",
    "    7:{\"A\":-0.48, \"R\":0.83, \"N\":-0.8, \"D\":0.56, \"C\":-0.19, \"Q\":-0.41, \"E\":0.02, \"G\":-1.34, \"H\":0.65, \"I\":-0.13, \"L\":-0.62, \"K\":0.13, \"M\":-0.68, \"F\":-0.2, \"P\":3.56, \"S\":0.11, \"T\":0.39, \"W\":-0.85, \"Y\":-0.52, \"V\":-0.03}\n",
    "}\n",
    "\n",
    "# Function info here ...\n",
    "def import_fa(file_name, return_labels=False):\n",
    "    print(\"Importing \"+file_name)\n",
    "    # List storage for file contents (sequences) and sequene labels\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    num_classes = 0\n",
    "    class_counts = [0]\n",
    "    # Try to open the file as read only\n",
    "    try:\n",
    "        file = open(file_name, \"r\")\n",
    "    # If the file would not open, print error message and exit\n",
    "    except:\n",
    "        print(\"\\nError: Could not open file\")\n",
    "        print(\"File name given: \"+file_name+\"\\n\")\n",
    "        raise SystemExit\n",
    "    # Iterate over all the lines in the file\n",
    "    for line in file.readlines():\n",
    "        # If this character is at the start of the line, it is a sequence label\n",
    "        if line[0] == \">\":\n",
    "            # If the argument 'return_labels' is True, append the label to a list\n",
    "            if return_labels:\n",
    "                labels.append(str(line)[1:-1])\n",
    "            else:\n",
    "                continue\n",
    "        elif \"class\" in line:\n",
    "            class_counts.append(0)\n",
    "            num_classes += 1\n",
    "        else:\n",
    "            class_counts[num_classes] += 1\n",
    "        # Append the sequence to the list of sequences\n",
    "        sequences.append(str(line)[:-1])\n",
    "    print(\"    \"+str(num_classes)+\" classes\")\n",
    "    print(\"    \"+str(len(sequences)-num_classes)+\" sequences\")\n",
    "    print(\"    \"+str(np.array(class_counts)[1:])+\" class counts\")\n",
    "    # If the argument 'return_labels' is True, return arrays of the sequences and labels\n",
    "    if return_labels:\n",
    "        return np.array(sequences), np.array(labels)\n",
    "    # If False, only return an array of the sequences\n",
    "    else:\n",
    "        return np.array(sequences)\n",
    "\n",
    "# Function info here ...\n",
    "def raw_to_features(data,resample=1.0,balance=False):\n",
    "    print(\"\\nConverting to features:\")\n",
    "    num_VHSE = len(VHSE_descriptors)\n",
    "    num_AA = len(amino_acids)\n",
    "    num_features = num_AA + num_VHSE\n",
    "    n = len(data)\n",
    "    # Find basic data stats for function use:\n",
    "    #   the max length sequence in the data\n",
    "    #   the number of classes\n",
    "    #   count of each class\n",
    "    max_seq_len = 0\n",
    "    class_counts = [0]\n",
    "    num_classes = -1\n",
    "    # Iterate over all sequences to count num classes, class frequencies, and the max sequence length\n",
    "    for line in data:\n",
    "        # Look out for a new class\n",
    "        if \"class\" in line:\n",
    "            class_counts.append(0)\n",
    "            num_classes += 1\n",
    "            continue\n",
    "        # Skip labels\n",
    "        elif line[0] == \">\":\n",
    "            continue\n",
    "        # Update max sequence length\n",
    "        if len(line) > max_seq_len:\n",
    "            max_seq_len = len(line)\n",
    "        # Update class counts\n",
    "        class_counts[num_classes+1] += 1\n",
    "    \n",
    "    # Since this was -1 based, increment to convert to 0 based count\n",
    "    num_classes += 1\n",
    "    n -= num_classes\n",
    "    \n",
    "    # Print stats found above\n",
    "    print(\"  Num Classes:\", num_classes)\n",
    "    print(\"  Class Counts:\")\n",
    "    frmt = \"{:>7}\"*num_classes\n",
    "    print(\" \"+frmt.format(*[i for i in range(num_classes)]))\n",
    "    print(\"   \",end=\"\")\n",
    "    frmt = \"{:>7}\"*num_classes\n",
    "    print(\" \"+frmt.format(*[class_counts[i] for i in range(1,num_classes+1)]))\n",
    "\n",
    "    # Split data by class for resampling\n",
    "    data_by_class = []\n",
    "    start_idx = 1\n",
    "    end_idx = start_idx + class_counts[1]\n",
    "    for i in range(num_classes):\n",
    "        new_class = data[start_idx+1:end_idx]\n",
    "        np.random.shuffle(new_class)\n",
    "        data_by_class.append( new_class )\n",
    "        if i < num_classes-1:\n",
    "          #print(start_idx, end_idx, class_counts[i+2], class_counts)\n",
    "          start_idx += end_idx\n",
    "          end_idx = start_idx + class_counts[i+2]\n",
    "\n",
    "    print(\"data by class:\",data_by_class)\n",
    "\n",
    "    # Init variables for resampling\n",
    "    total_per_class = 0\n",
    "    num_samples = n\n",
    "    # If resampling:\n",
    "    if resample != 1.0:\n",
    "        # Calculate the size of the resampled dataset\n",
    "        num_samples = int(n*resample)\n",
    "        # If balanced classs coutns are required:\n",
    "        if balance:\n",
    "            # Calculate a balanced total number of instances per class\n",
    "            total_per_class = int(num_samples/num_classes)\n",
    "            if total_per_class > min(class_counts):\n",
    "                total_per_class = min(class_counts)\n",
    "            #print(\"\\ntotal/class:\", total_per_class, \"\\n\")\n",
    "            for i in range(len(data_by_class)):\n",
    "                indexes = np.random.default_rng().choice(len(data_by_class[i]), size=total_per_class, replace=False)\n",
    "                #print(\"idxs:\",indexes)\n",
    "                data_by_class[i] = data_by_class[i][indexes]\n",
    "            \n",
    "        # If imbalanced class counts are acceptable\n",
    "        else:\n",
    "            # Calculate the total number of instances per class in the same ratio as class counts\n",
    "            total_per_class = [int(num_samples*class_counts[i+1]/n) for i in range(num_classes)]\n",
    "            #print(\"total/class:\",total_per_class)\n",
    "            for i in range(len(data_by_class)):\n",
    "                indexes = np.random.default_rng().choice(len(data_by_class[i]), size=total_per_class[i], replace=False)\n",
    "                data_by_class[i] = data_by_class[i][indexes]\n",
    "            \n",
    "        print(\"  Resampling:\")\n",
    "        print(\"    Dataset size :\",n)\n",
    "        print(\"    Total samples:\",num_samples)\n",
    "        print(\"    Total/class  :\",total_per_class)\n",
    "        print()\n",
    "    # If not resampling\n",
    "    # TODO:  Complete - could the above be used instead of conditional?\n",
    "    else:\n",
    "        total_per_class = int(n/num_classes)\n",
    "    \n",
    "    #print(\"data by class:\",data_by_class)\n",
    "    \n",
    "    # Instantiate storage of features and labels\n",
    "    X = []\n",
    "    y = []\n",
    "      \n",
    "    # Convert from strings to arrays and pad the sequences with zeros to \n",
    "    # the length of the maximum length sequence; this is the number of features\n",
    "    index = 0\n",
    "    for i in range(len(data_by_class)):\n",
    "        for j in range(len(data_by_class[i])):\n",
    "            # Instantiate new array of zeros\n",
    "            new_instance = []\n",
    "            for k in range(num_VHSE):\n",
    "                new_instance.append([-999 for acid in range(max_seq_len)])\n",
    "            for k in range(num_AA):\n",
    "                new_instance.append([0 for acid in range(max_seq_len)])\n",
    "            # Fill in sequence\n",
    "            for k in range(len(data_by_class[i][j])):\n",
    "                acid_char = data_by_class[i][j][k]\n",
    "                if acid_char in amino_acids:\n",
    "                    for l in range(num_VHSE):\n",
    "                        new_instance[l][k] = VHSE_descriptors[l][acid_char]\n",
    "                    new_instance[amino_acids[acid_char]+num_VHSE][k] = 1\n",
    "            # Store new sequence in features store X\n",
    "            X.append(new_instance)\n",
    "            y.append(i)\n",
    "            index += 1\n",
    "\n",
    "    # Return features and labels\n",
    "    return np.array(X),np.array(y),max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5d7ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For one-hot encoding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# For the loss function and optimizer\n",
    "import tensorflow.keras as keras\n",
    "# For the model\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8b990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data and extract features\n",
    "import time\n",
    "start = time.time()\n",
    "data = import_fa(\"data/pos_neg-unip.fa\")\n",
    "X, y, seq_len = raw_to_features(data, resample=0.2)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time taken:\", round(end-start,3),\"sec\")\n",
    "print(\"shape of X:\",np.shape(X))\n",
    "print(\"shape of y:\",np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a07841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y to one-hot encoding for the CNN \n",
    "y_one_hot = to_categorical(y)\n",
    "\n",
    "# Split the data into train, test, and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, valid_X, y_one_hot, valid_y = train_test_split(X, y_one_hot, test_size=0.2, random_state=0)\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y_one_hot, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc0cd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_class_counts = [0,0]\n",
    "for i in range(len(y_one_hot)):\n",
    "  for j in range(len(y_one_hot[0])):\n",
    "    if y_one_hot[i][j]==1:\n",
    "      train_class_counts[j] += 1\n",
    "valid_class_counts = [0,0]\n",
    "for i in range(len(valid_y)):\n",
    "  for j in range(len(valid_y[0])):\n",
    "    if valid_y[i][j]==1:\n",
    "      valid_class_counts[j] += 1\n",
    "\n",
    "# Number of data instances\n",
    "n = len(train_X)\n",
    "# Number of features\n",
    "m = len(train_X[0])\n",
    "# Length of sequence\n",
    "o = len(train_X[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5491a0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print some data info\n",
    "print(\"Shape of X      :\",np.shape(X))\n",
    "print(\"Shape of train_X:\",np.shape(train_X))\n",
    "print(\"Shape of valid_X:\", np.shape(valid_X))\n",
    "print()\n",
    "print(\"Class distributions:\")\n",
    "print(\"  Train/Test:\",train_class_counts)\n",
    "print(\"  Validation:\", valid_class_counts)\n",
    "print()\n",
    "print('n (no. train samples):',n)\n",
    "print('m (num features)     :',m)\n",
    "print('o (length of seqs)   :',o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7e5f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some model parameters\n",
    "batch_size = 64\n",
    "epochs = 30\n",
    "num_classes = 2\n",
    "\n",
    "# Instantiate the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers\n",
    "model.add(Conv2D(filters=32, kernel_size=(m,3),activation='linear',input_shape=(m,o,1),padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=2,padding='same'))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(1,3),activation='linear',padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=2,padding='same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "# For now, this gives errors about loading the dynamic library for GPU use\n",
    "model.compile(\n",
    "    loss=keras.losses.categorical_crossentropy, \n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ec3109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify log files\n",
    "#from datetime import datetime\n",
    "#logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_X, train_y, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44cdb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"21.12.13_model_imbalanced\")\n",
    "\n",
    "# Print some of the results\n",
    "import matplotlib.pyplot as plt\n",
    "# Loss over epochs\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Test')\n",
    "plt.title('MAE')\n",
    "plt.ylabel('MAE value')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Accuracy over epochs\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Test Accuracy')\n",
    "plt.title('% Classification Accuracy')\n",
    "plt.ylabel('% Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f94e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of validation samples:\",len(valid_X))\n",
    "# Evaluate the model with the final evaluation testing set\n",
    "results = model.evaluate(valid_X, valid_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db85d9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data and extract features\n",
    "import time\n",
    "start = time.time()\n",
    "data = import_fa(\"data/pos_neg.fa\")\n",
    "X, y, seq_len = raw_to_features(data, resample=0.2, balance=True)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time taken:\", round(end-start,3),\"sec\")\n",
    "print(\"shape of X:\",np.shape(X))\n",
    "print(\"shape of y:\",np.shape(y))\n",
    "\n",
    "# Convert y to one-hot encoding for the CNN \n",
    "y_one_hot = to_categorical(y)\n",
    "\n",
    "# Split the data into train, test, and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, valid_X, y_one_hot, valid_y = train_test_split(X, y_one_hot, test_size=0.2, random_state=0)\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y_one_hot, test_size=0.2, random_state=0)\n",
    "\n",
    "# Get params\n",
    "train_class_counts = [0,0]\n",
    "for i in range(len(y_one_hot)):\n",
    "  for j in range(len(y_one_hot[0])):\n",
    "    if y_one_hot[i][j]==1:\n",
    "      train_class_counts[j] += 1\n",
    "valid_class_counts = [0,0]\n",
    "for i in range(len(valid_y)):\n",
    "  for j in range(len(valid_y[0])):\n",
    "    if valid_y[i][j]==1:\n",
    "      valid_class_counts[j] += 1\n",
    "\n",
    "# Number of data instances\n",
    "n = len(train_X)\n",
    "# Number of features\n",
    "m = len(train_X[0])\n",
    "# Length of sequence\n",
    "o = len(train_X[0][0])\n",
    "\n",
    "# Print some data info\n",
    "print(\"Shape of X      :\",np.shape(X))\n",
    "print(\"Shape of train_X:\",np.shape(train_X))\n",
    "print(\"Shape of valid_X:\", np.shape(valid_X))\n",
    "print()\n",
    "print(\"Class distributions:\")\n",
    "print(\"  Train/Test:\",train_class_counts)\n",
    "print(\"  Validation:\", valid_class_counts)\n",
    "print()\n",
    "print('n (no. train samples):',n)\n",
    "print('m (num features)     :',m)\n",
    "print('o (length of seqs)   :',o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7dec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some model parameters\n",
    "batch_size = 64\n",
    "epochs = 30\n",
    "num_classes = 2\n",
    "\n",
    "# Instantiate the model\n",
    "model_2 = Sequential()\n",
    "\n",
    "# Add layers\n",
    "model_2.add(Conv2D(filters=32, kernel_size=(m,3),activation='linear',input_shape=(m,o,1),padding='same'))\n",
    "model_2.add(LeakyReLU(alpha=0.1))\n",
    "model_2.add(MaxPooling2D(pool_size=2,padding='same'))\n",
    "\n",
    "model_2.add(Conv2D(filters=64, kernel_size=(1,3),activation='linear',padding='same'))\n",
    "model_2.add(LeakyReLU(alpha=0.1))\n",
    "model_2.add(MaxPooling2D(pool_size=2,padding='same'))\n",
    "\n",
    "model_2.add(Flatten())\n",
    "model_2.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model_2\n",
    "# For now, this gives errors about loading the dynamic library for GPU use\n",
    "model_2.compile(\n",
    "    loss=keras.losses.categorical_crossentropy, \n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Print a summary of the model_2\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ba9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify log files\n",
    "#from datetime import datetime\n",
    "#logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "# Train the model\n",
    "history = model_2.fit(train_X, train_y, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec73279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_2.save(\"21.12.13_model_2\")\n",
    "\n",
    "# Print some of the results\n",
    "import matplotlib.pyplot as plt\n",
    "# Loss over epochs\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Test')\n",
    "plt.title('MAE')\n",
    "plt.ylabel('MAE value')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Accuracy over epochs\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Test Accuracy')\n",
    "plt.title('% Classification Accuracy')\n",
    "plt.ylabel('% Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df61d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of validation samples:\",len(valid_X))\n",
    "# Evaluate the model with the final evaluation testing set\n",
    "results = model_2.evaluate(valid_X, valid_y)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
